{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation of models and data"
      ],
      "metadata": {
        "id": "t2CDge2Tlv0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator (GPT-2), Retriever (Sentence Transformer)"
      ],
      "metadata": {
        "id": "9vfn-23vSXtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest transformers library\n",
        "!pip install transformers -U\n",
        "!pip3 install faiss-gpu\n",
        "\n",
        "# # Download the DailyDialog dataset\n",
        "# !wget https://www.dropbox.com/scl/fi/ai4je7bp3difjeuyk3a8s/dailydialog.json?rlkey=fe6pm2iz7nsb5fwulsjch5qzy -O dailydialog.json"
      ],
      "metadata": {
        "id": "blPcWElsl4GG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c9a8e6-16d1-459f-e5b0-fbe02cd9b426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoModel, AutoTokenizer\n",
        "import torch\n",
        "import shutil\n",
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "6hktxLpCK5Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##GPT2- Generator\n",
        "# Load a GPT2 tokenizer\n",
        "generator_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# Add special token <SEP> and <PAD>\n",
        "generator_tokenizer.add_special_tokens({\"sep_token\": \"<SEP>\", \"pad_token\": \"<PAD>\"})\n",
        "\n",
        "generator_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "generator_model.resize_token_embeddings(len(generator_tokenizer))\n",
        "\n",
        "##Sentence Transformer- Retriever #maximum length of input is 512\n",
        "retriever_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "retriever_tokenizer.add_special_tokens({\"sep_token\": \"<SEP>\", \"pad_token\": \"<PAD>\"})\n",
        "#30524\n",
        "retriever_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "retriever_model.resize_token_embeddings(len(retriever_tokenizer))"
      ],
      "metadata": {
        "id": "l0PPd3relxWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1bd35b-138d-4b95-8b2f-a6efceff9444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30524, 384, padding_idx=0)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "generator_model = generator_model.to(device)\n",
        "retriever_model = retriever_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XojTKPFNu04",
        "outputId": "b1aa9d6e-f928-4d25-dccd-d72e821e350f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passages with pubtator search"
      ],
      "metadata": {
        "id": "ORF8ejpPSfoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "b3Xm8E6jSVKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_string_after(substring, text):\n",
        "    parts = text.split(substring, 1)  # Split at the first occurrence of substring\n",
        "    if len(parts) > 1:\n",
        "        return parts[1]  # Return the text after the substring\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def query_to_pmids(query, page_num=1):\n",
        "    #Part 1. query to list of pmids\n",
        "    pmids=[]\n",
        "    for i in range(page_num):\n",
        "        url=\"https://www.ncbi.nlm.nih.gov/research/pubtator3-api/search/?text=\"+query+\"&page=\"+str(i+1)\n",
        "        response = requests.get(url)\n",
        "        json_dict=json.loads(response.text)\n",
        "        for j in range(len(json_dict['results'])):\n",
        "            pmids.append(json_dict['results'][j]['pmid'])\n",
        "    return pmids\n",
        "\n",
        "def pmid_to_text(pmid):\n",
        "    url=\"https://www.ncbi.nlm.nih.gov/research/pubtator3-api/publications/export/pubtator?pmids=\"+str(pmid)\n",
        "    response = requests.get(url)\n",
        "    data_lines=response.text.splitlines()\n",
        "    text=get_string_after('|a|',data_lines[1])\n",
        "    return text"
      ],
      "metadata": {
        "id": "J5Z94RtsSfPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmids=query_to_pmids('Breast Cancer',10)\n",
        "passages=[text for x in pmids if len(text:=pmid_to_text(x))>5]\n",
        "print(len(passages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5ixuo7YSt3t",
        "outputId": "8d39d692-0f82-43d2-9bb1-cd212496d9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "length_text=[len(text.split(' ')) for text in passages]\n",
        "plt.hist(length_text)\n",
        "#Most of them are shorter than 512 words\n",
        "#In terms of token, the text can be longer but it seems okay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "Uy4jLtAfStio",
        "outputId": "457025c7-08fe-40eb-cf70-2056abd82964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([13., 12., 10., 28.,  6.,  2.,  0.,  0.,  0.,  1.]),\n",
              " array([ 67. , 126.2, 185.4, 244.6, 303.8, 363. , 422.2, 481.4, 540.6,\n",
              "        599.8, 659. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbRElEQVR4nO3de5DVZf3A8c/SyhGSXQLcXTYXxLTQEDIx2jSz2BHJsSymUbMZNKdGWyqim3TRnC5rNWPWDOF0g5oyu0xg3jCCWLIAgySkC4FhULpQOuwC5oru8/vD8fw6guiBs8+y2+s1853hfL/PnvOcZ5az7/meW1VKKQUAQCaD+noCAMD/FvEBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZVff1BJ6tp6cnHnrooRg2bFhUVVX19XQAgBcgpRS7d++OxsbGGDTo4Oc2jrj4eOihh6KpqamvpwEAHILt27fHcccdd9AxR1x8DBs2LCKennxNTU0fzwYAeCG6urqiqamp+Hf8YI64+HjmqZaamhrxAQD9zAt5yYQXnAIAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAsqru6wnAkeL4q+/o6ymU7cHrz+/rKQCUzZkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgq7Lio62tLc4444wYNmxY1NXVxYUXXhibNm0qGXPOOedEVVVVyXbllVdWdNIAQP9VVny0t7dHa2trrF69OpYuXRr79u2Lc889N/bu3Vsy7j3veU88/PDDxe1LX/pSRScNAPRf1eUMXrJkScnlhQsXRl1dXaxbty7OPvvs4v6hQ4dGQ0NDZWYIAAwoh/Waj87OzoiIGDFiRMn+H/zgBzFq1KiYMGFCzJ07Nx577LHnvI7u7u7o6uoq2QCAgausMx//raenJ2bPnh1nnnlmTJgwobj/ne98Z4wdOzYaGxtjw4YN8fGPfzw2bdoUP/vZzw54PW1tbXHdddcd6jQAgH6mKqWUDuUHr7rqqrjrrrvinnvuieOOO+45xy1fvjymTp0aW7ZsiZe97GX7He/u7o7u7u7i5a6urmhqaorOzs6oqak5lKnBITn+6jv6egple/D68/t6CgAR8fTf79ra2hf09/uQznzMmjUrbr/99li5cuVBwyMiYsqUKRERzxkfhUIhCoXCoUwDAOiHyoqPlFK8//3vj0WLFsWKFSti3Lhxz/sz69evj4iI0aNHH9IEAYCBpaz4aG1tjZtvvjluvfXWGDZsWHR0dERERG1tbQwZMiQeeOCBuPnmm+PNb35zjBw5MjZs2BAf+tCH4uyzz46JEyf2yh0AAPqXsuJj/vz5EfH0B4n9twULFsRll10WgwcPjl/+8pdx4403xt69e6OpqSlmzJgRn/rUpyo2YQCgfyv7aZeDaWpqivb29sOaEAAwsPluFwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJBVWfHR1tYWZ5xxRgwbNizq6uriwgsvjE2bNpWMefzxx6O1tTVGjhwZxxxzTMyYMSN27NhR0UkDAP1XWfHR3t4era2tsXr16li6dGns27cvzj333Ni7d29xzIc+9KG47bbb4ic/+Um0t7fHQw89FG9/+9srPnEAoH+qLmfwkiVLSi4vXLgw6urqYt26dXH22WdHZ2dnfPvb346bb7453vSmN0VExIIFC+Lkk0+O1atXx2tf+9rKzRwA6JcO6zUfnZ2dERExYsSIiIhYt25d7Nu3L1paWopjxo8fH2PGjIlVq1Yd8Dq6u7ujq6urZAMABq5Djo+enp6YPXt2nHnmmTFhwoSIiOjo6IjBgwfH8OHDS8bW19dHR0fHAa+nra0tamtri1tTU9OhTgkA6AcOOT5aW1tj48aNccsttxzWBObOnRudnZ3Fbfv27Yd1fQDAka2s13w8Y9asWXH77bfHypUr47jjjivub2hoiCeeeCJ27dpVcvZjx44d0dDQcMDrKhQKUSgUDmUaAEA/VNaZj5RSzJo1KxYtWhTLly+PcePGlRw//fTT46ijjoply5YV923atCm2bdsWzc3NlZkxANCvlXXmo7W1NW6++ea49dZbY9iwYcXXcdTW1saQIUOitrY2rrjiipgzZ06MGDEiampq4v3vf380Nzd7pwsAEBFlxsf8+fMjIuKcc84p2b9gwYK47LLLIiLiK1/5SgwaNChmzJgR3d3dMW3atPj6179ekckCAP1fWfGRUnreMUcffXTMmzcv5s2bd8iTAgAGLt/tAgBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALIqOz5WrlwZF1xwQTQ2NkZVVVUsXry45Phll10WVVVVJdt5551XqfkCAP1c2fGxd+/emDRpUsybN+85x5x33nnx8MMPF7cf/vCHhzVJAGDgqC73B6ZPnx7Tp08/6JhCoRANDQ2HPCkAYODqldd8rFixIurq6uIVr3hFXHXVVfHII48859ju7u7o6uoq2QCAgavi8XHeeefF9773vVi2bFl88YtfjPb29pg+fXo89dRTBxzf1tYWtbW1xa2pqanSUwIAjiBlP+3yfC6++OLiv0899dSYOHFivOxlL4sVK1bE1KlT9xs/d+7cmDNnTvFyV1eXAAGAAazX32p7wgknxKhRo2LLli0HPF4oFKKmpqZkAwAGrl6Pj3/84x/xyCOPxOjRo3v7pgCAfqDsp1327NlTchZj69atsX79+hgxYkSMGDEirrvuupgxY0Y0NDTEAw88EB/72MfixBNPjGnTplV04gBA/1R2fKxduzbe+MY3Fi8/83qNmTNnxvz582PDhg3x3e9+N3bt2hWNjY1x7rnnxmc/+9koFAqVmzUA0G+VHR/nnHNOpJSe8/jdd999WBMCAAY23+0CAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsqvt6Arkdf/UdfT2Fsj14/fl9PQUAqBhnPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALL6n/tul/7I99EAMJA48wEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALIqOz5WrlwZF1xwQTQ2NkZVVVUsXry45HhKKa655poYPXp0DBkyJFpaWmLz5s2Vmi8A0M+VHR979+6NSZMmxbx58w54/Etf+lJ87Wtfi5tuuinWrFkTL37xi2PatGnx+OOPH/ZkAYD+r7rcH5g+fXpMnz79gMdSSnHjjTfGpz71qXjrW98aERHf+973or6+PhYvXhwXX3zx4c0WAOj3Kvqaj61bt0ZHR0e0tLQU99XW1saUKVNi1apVlbwpAKCfKvvMx8F0dHRERER9fX3J/vr6+uKxZ+vu7o7u7u7i5a6urkpOCQA4wlQ0Pg5FW1tbXHfddX09DSrs+Kvv6OspAHCEqujTLg0NDRERsWPHjpL9O3bsKB57trlz50ZnZ2dx2759eyWnBAAcYSoaH+PGjYuGhoZYtmxZcV9XV1esWbMmmpubD/gzhUIhampqSjYAYOAq+2mXPXv2xJYtW4qXt27dGuvXr48RI0bEmDFjYvbs2fG5z30uTjrppBg3blx8+tOfjsbGxrjwwgsrOW8AoJ8qOz7Wrl0bb3zjG4uX58yZExERM2fOjIULF8bHPvax2Lt3b7z3ve+NXbt2xVlnnRVLliyJo48+unKzBgD6raqUUurrSfy3rq6uqK2tjc7Ozl55CsYLIRlIHrz+/L6eAkBElPf323e7AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArKr7egLAoTv+6jv6egple/D68/t6CkAfc+YDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyqnh8fOYzn4mqqqqSbfz48ZW+GQCgn+qVDxl75StfGb/85S///0aqfZYZAPC0XqmC6urqaGho6I2rBgD6uV55zcfmzZujsbExTjjhhLj00ktj27Ztzzm2u7s7urq6SjYAYOCqeHxMmTIlFi5cGEuWLIn58+fH1q1b4/Wvf33s3r37gOPb2tqitra2uDU1NVV6SgDAEaQqpZR68wZ27doVY8eOjRtuuCGuuOKK/Y53d3dHd3d38XJXV1c0NTVFZ2dn1NTUVHw+/fGLuGAg8cVyMDB1dXVFbW3tC/r73euvBB0+fHi8/OUvjy1bthzweKFQiEKh0NvTAACOEL3+OR979uyJBx54IEaPHt3bNwUA9AMVj4+PfOQj0d7eHg8++GD89re/jbe97W3xohe9KC655JJK3xQA0A9V/GmXf/zjH3HJJZfEI488Escee2ycddZZsXr16jj22GMrfVMAQD9U8fi45ZZbKn2VAMAA4rtdAICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgq+q+ngDwv+X4q+/o6ymU7cHrz+/rKcCA4swHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMjKF8sBwGHwZYnlc+YDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACy6rX4mDdvXhx//PFx9NFHx5QpU+Lee+/trZsCAPqRXomPH/3oRzFnzpy49tpr4/e//31MmjQppk2bFjt37uyNmwMA+pFeiY8bbrgh3vOe98Tll18ep5xyStx0000xdOjQ+M53vtMbNwcA9CMV/3j1J554ItatWxdz584t7hs0aFC0tLTEqlWr9hvf3d0d3d3dxcudnZ0REdHV1VXpqUVERE/3Y71yvcDA1VuPRwwM/fHvSm/8Tj9znSml5x1b8fj497//HU899VTU19eX7K+vr4+//OUv+41va2uL6667br/9TU1NlZ4awCGpvbGvZwCV1Zu/07t3747a2tqDjunzL5abO3duzJkzp3i5p6cnHn300Rg5cmRUVVX14cwOTVdXVzQ1NcX27dujpqamr6dzxLNe5bFe5bNm5bFe5bFe/y+lFLt3747GxsbnHVvx+Bg1alS86EUvih07dpTs37FjRzQ0NOw3vlAoRKFQKNk3fPjwSk8ru5qamv/5X8RyWK/yWK/yWbPyWK/yWK+nPd8Zj2dU/AWngwcPjtNPPz2WLVtW3NfT0xPLli2L5ubmSt8cANDP9MrTLnPmzImZM2fG5MmT4zWveU3ceOONsXfv3rj88st74+YAgH6kV+Ljoosuin/9619xzTXXREdHR7zqVa+KJUuW7Pci1IGoUCjEtddeu99TSRyY9SqP9SqfNSuP9SqP9To0VemFvCcGAKBCfLcLAJCV+AAAshIfAEBW4gMAyEp8vAArV66MCy64IBobG6OqqioWL15ccjylFNdcc02MHj06hgwZEi0tLbF58+aSMY8++mhceumlUVNTE8OHD48rrrgi9uzZk/Fe5NPW1hZnnHFGDBs2LOrq6uLCCy+MTZs2lYx5/PHHo7W1NUaOHBnHHHNMzJgxY78Pptu2bVucf/75MXTo0Kirq4uPfvSj8eSTT+a8K1nMnz8/Jk6cWPyQoubm5rjrrruKx63VwV1//fVRVVUVs2fPLu6zZqU+85nPRFVVVck2fvz44nHrtb9//vOf8a53vStGjhwZQ4YMiVNPPTXWrl1bPO5x/zAlntedd96ZPvnJT6af/exnKSLSokWLSo5ff/31qba2Ni1evDj94Q9/SG95y1vSuHHj0n/+85/imPPOOy9NmjQprV69Ov36179OJ554Yrrkkksy35M8pk2blhYsWJA2btyY1q9fn9785jenMWPGpD179hTHXHnllampqSktW7YsrV27Nr32ta9Nr3vd64rHn3zyyTRhwoTU0tKS7rvvvnTnnXemUaNGpblz5/bFXepVP//5z9Mdd9yR/vrXv6ZNmzalT3ziE+moo45KGzduTClZq4O599570/HHH58mTpyYPvjBDxb3W7NS1157bXrlK1+ZHn744eL2r3/9q3jcepV69NFH09ixY9Nll12W1qxZk/72t7+lu+++O23ZsqU4xuP+4REfZXp2fPT09KSGhob05S9/ubhv165dqVAopB/+8IcppZT+9Kc/pYhIv/vd74pj7rrrrlRVVZX++c9/Zpt7X9m5c2eKiNTe3p5Senp9jjrqqPSTn/ykOObPf/5zioi0atWqlNLTwTdo0KDU0dFRHDN//vxUU1OTuru7896BPvCSl7wkfetb37JWB7F79+500kknpaVLl6Y3vOENxfiwZvu79tpr06RJkw54zHrt7+Mf/3g666yznvO4x/3D52mXw7R169bo6OiIlpaW4r7a2tqYMmVKrFq1KiIiVq1aFcOHD4/JkycXx7S0tMSgQYNizZo12eecW2dnZ0REjBgxIiIi1q1bF/v27StZs/Hjx8eYMWNK1uzUU08t+WC6adOmRVdXV/zxj3/MOPu8nnrqqbjlllti79690dzcbK0OorW1Nc4///yStYnw+/VcNm/eHI2NjXHCCSfEpZdeGtu2bYsI63UgP//5z2Py5Mnxjne8I+rq6uK0006Lb37zm8XjHvcPn/g4TB0dHRER+316a319ffFYR0dH1NXVlRyvrq6OESNGFMcMVD09PTF79uw488wzY8KECRHx9HoMHjx4vy8QfPaaHWhNnzk20Nx///1xzDHHRKFQiCuvvDIWLVoUp5xyirV6Drfcckv8/ve/j7a2tv2OWbP9TZkyJRYuXBhLliyJ+fPnx9atW+P1r3997N6923odwN/+9reYP39+nHTSSXH33XfHVVddFR/4wAfiu9/9bkR43K+EXvl4dXhGa2trbNy4Me65556+nsoR7RWveEWsX78+Ojs746c//WnMnDkz2tvb+3paR6Tt27fHBz/4wVi6dGkcffTRfT2dfmH69OnFf0+cODGmTJkSY8eOjR//+McxZMiQPpzZkamnpycmT54cX/jCFyIi4rTTTouNGzfGTTfdFDNnzuzj2Q0MznwcpoaGhoiI/V4ZvmPHjuKxhoaG2LlzZ8nxJ598Mh599NHimIFo1qxZcfvtt8evfvWrOO6444r7Gxoa4oknnohdu3aVjH/2mh1oTZ85NtAMHjw4TjzxxDj99NOjra0tJk2aFF/96let1QGsW7cudu7cGa9+9aujuro6qquro729Pb72ta9FdXV11NfXW7PnMXz48Hj5y18eW7Zs8Tt2AKNHj45TTjmlZN/JJ59cfKrK4/7hEx+Hady4cdHQ0BDLli0r7uvq6oo1a9ZEc3NzREQ0NzfHrl27Yt26dcUxy5cvj56enpgyZUr2Ofe2lFLMmjUrFi1aFMuXL49x48aVHD/99NPjqKOOKlmzTZs2xbZt20rW7P777y/5z7t06dKoqanZ70FhIOrp6Ynu7m5rdQBTp06N+++/P9avX1/cJk+eHJdeemnx39bs4Pbs2RMPPPBAjB492u/YAZx55pn7fTzAX//61xg7dmxEeNyviL5+xWt/sHv37nTfffel++67L0VEuuGGG9J9992X/v73v6eUnn7L1fDhw9Ott96aNmzYkN761rce8C1Xp512WlqzZk2655570kknnTRg33J11VVXpdra2rRixYqSt/Y99thjxTFXXnllGjNmTFq+fHlau3Ztam5uTs3NzcXjz7y179xzz03r169PS5YsSccee+yAfGvf1Vdfndrb29PWrVvThg0b0tVXX52qqqrSL37xi5SStXoh/vvdLilZs2f78Ic/nFasWJG2bt2afvOb36SWlpY0atSotHPnzpSS9Xq2e++9N1VXV6fPf/7zafPmzekHP/hBGjp0aPr+979fHONx//CIjxfgV7/6VYqI/baZM2emlJ5+29WnP/3pVF9fnwqFQpo6dWratGlTyXU88sgj6ZJLLknHHHNMqqmpSZdffnnavXt3H9yb3negtYqItGDBguKY//znP+l973tfeslLXpKGDh2a3va2t6WHH3645HoefPDBNH369DRkyJA0atSo9OEPfzjt27cv873pfe9+97vT2LFj0+DBg9Oxxx6bpk6dWgyPlKzVC/Hs+LBmpS666KI0evToNHjw4PTSl740XXTRRSWfWWG99nfbbbelCRMmpEKhkMaPH5++8Y1vlBz3uH94qlJKqW/OuQAA/4u85gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZPV//+3KEiTuldoAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define necessary functions"
      ],
      "metadata": {
        "id": "SbqzpDn6WPsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_embedder(texts, model, tokenizer):\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    inputs.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**inputs).last_hidden_state[:,0,:]\n",
        "    return embeddings.cpu().numpy()"
      ],
      "metadata": {
        "id": "l4tFxpdfOEse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "passage_embeddings = text_embedder(passages, retriever_model, retriever_tokenizer, )\n",
        "# 72 x 384\n",
        "index = faiss.IndexFlatL2(passage_embeddings.shape[1])\n",
        "#Build FAISS index for passage embeddings\n",
        "index.add(passage_embeddings)"
      ],
      "metadata": {
        "id": "Dd-f0nAjWVbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "passage_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8w8Xhdlf-S5",
        "outputId": "909dc036-6322-4f5f-99ce-0dbb6e167acc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(72, 384)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_passages(query, passages, k=1):\n",
        "    query_embedding = text_embedder([query], retriever_model, retriever_tokenizer)\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    return [passages[i] for i in indices[0]]"
      ],
      "metadata": {
        "id": "3CC7_9nIXF8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query='I have breast cancer. Should I smoke?'\n",
        "\n",
        "embedding_query = text_embedder([query], retriever_model, retriever_tokenizer)\n",
        "# 1 x 384, B x dim\n",
        "\n",
        "retrieved_passages = retrieve_passages(query, passages)"
      ],
      "metadata": {
        "id": "r5A9ktEeO89s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_passages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQetOQGmYuo5",
        "outputId": "da83f10f-bf5a-484c-8e7a-65b0f3c0fc12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Introduction: breast cancer is the most commonly diagnosed malignancy and an important cause of cancer death among females worldwide. The disease accounted for 25% (1.67 million) of new cancer cases and the fifth cause of cancer deaths. Incidence of all types of cancers is approximately 25% in Sierra Leone. However, there was no documented evidence on risk factors for breast cancer among women in the country. The main aim of this study was to assess risk factors associated with breast cancer among women screened for breast cancer in Freetown Sierra Leone. Methods: we conducted a case-control study on breast cancer involving 116 confirmed breast cancer cases and 116 controls. Questionnaire was designed to collect data on socio-demographic, reproductive and behavioral risk factors. Analysis was carried using logistic regression to assess the associations between breast cancer and the risk factors. Results: in the final multiple logistic regression, had formal educational level, (aOR 0.1, 0.03-0.26, p= 0.001) physical activity for more than 30 minutes per week (aOR 0.5 (0.9- 0.7, p=0.001). Cigarette smoking (aOR 4.8, 1.2-18.5, p=0.023) and family history of breast cancer (aOR 9.9 cigarette smoking (OR 4.4, 1.2-18.5, p=0.023) and family history of breast cancer (OR 9.9, 2.7-36.45, p=0.040) were identified as the main risk factors for breast cancer. This study did not find any statistically significant associations between reproductive risk factors and breast cancer. Conclusion: risk factors for breast cancer among women in Sierra Leone include educational level, physical activity, cigarette smoking and family history of breast cancer. We recommended screening program for women above 40 years and had history of breast cancer. Also, to establish breast cancer registry.']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \" \".join(retrieved_passages)\n",
        "input_text = f\"{context}\\nQuery: {query}\\nAnswer:\"\n",
        "inputs = generator_tokenizer(input_text, return_tensors='pt', truncation=True)"
      ],
      "metadata": {
        "id": "wpm01KHZhXl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = generator_model.generate(**inputs.to(device), max_length=500, num_return_sequences=1)\n",
        "# response = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "811yZSonhZ1X",
        "outputId": "c04da803-1087-4f2c-ee7a-10b42eda3703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CmhocqIWsWMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbUdYQNnqs7U",
        "outputId": "763f7e10-f74f-4fc7-89df-5ee8b0168232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 500])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "O_Cza8PvrlWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P4zcHWdMr0sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "nWXnbeBvrnDr",
        "outputId": "370136de-1c90-4231-9394-06378e3cc35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Introduction: breast cancer is the most commonly diagnosed malignancy and an important cause of cancer death among females worldwide. The disease accounted for 25% (1.67 million) of new cancer cases and the fifth cause of cancer deaths. Incidence of all types of cancers is approximately 25% in Sierra Leone. However, there was no documented evidence on risk factors for breast cancer among women in the country. The main aim of this study was to assess risk factors associated with breast cancer among women screened for breast cancer in Freetown Sierra Leone. Methods: we conducted a case-control study on breast cancer involving 116 confirmed breast cancer cases and 116 controls. Questionnaire was designed to collect data on socio-demographic, reproductive and behavioral risk factors. Analysis was carried using logistic regression to assess the associations between breast cancer and the risk factors. Results: in the final multiple logistic regression, had formal educational level, (aOR 0.1, 0.03-0.26, p= 0.001) physical activity for more than 30 minutes per week (aOR 0.5 (0.9- 0.7, p=0.001). Cigarette smoking (aOR 4.8, 1.2-18.5, p=0.023) and family history of breast cancer (aOR 9.9 cigarette smoking (OR 4.4, 1.2-18.5, p=0.023) and family history of breast cancer (OR 9.9, 2.7-36.45, p=0.040) were identified as the main risk factors for breast cancer. This study did not find any statistically significant associations between reproductive risk factors and breast cancer. Conclusion: risk factors for breast cancer among women in Sierra Leone include educational level, physical activity, cigarette smoking and family history of breast cancer. We recommended screening program for women above 40 years and had history of breast cancer. Also, to establish breast cancer registry.\\nQuery: I have breast cancer. Should I smoke?\\nAnswer: Yes, you should smoke.\\nQuestionnaire: I have breast cancer. Should I smoke?\\nAnswer: Yes, you should smoke.\\nQuestionnaire: I have breast cancer. Should I smoke?\\nQuestionnaire: I have breast cancer. Should I smoke?\\nQuestionnaire: I have breast cancer. Should I smoke?\\nQuestionnaire: I have breast cancer. Should I smoke?\\nQuestionnaire: I have'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_pipeline(query):\n",
        "    retrieved_passages = retrieve_passages(query, passages)\n",
        "    context = \" \".join(retrieved_passages)\n",
        "\n",
        "    input_text = f\"{context}\\nQuery: {query}\\nAnswer:\"\n",
        "    inputs = generator_tokenizer(input_text, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = generator_model.generate(**inputs.to(device), max_length=2000, num_return_sequences=1)\n",
        "    response = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "def rag_no_passage(query):\n",
        "\n",
        "    input_text = f\"Query: {query}\\nAnswer:\"\n",
        "    inputs = generator_tokenizer(input_text, return_tensor='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = generator_model.generate(**inputs.to(device), max_length=2000, num_return_sequences=1)\n",
        "    response = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ],
      "metadata": {
        "id": "RNchCZbEQeWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_rag = rag_pipeline(query)\n",
        "response_norag= rag_no_passage(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "uB-E4IOyc8si",
        "outputId": "86a19340-d181-43a1-96e5-c84c4b656f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (1024). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-154990e32627>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse_rag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresponse_norag\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrag_no_passage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-24d645ee6f04>\u001b[0m in \u001b[0;36mrag_pipeline\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2216\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1272\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m         \u001b[0mposition_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(utterances):\n",
        "    \"\"\"\n",
        "    [\"Hello\", \"Hi, how are you?\", \"Great!\"]\n",
        "      --> \"Hello<SEP>Hi, how are you?<SEP>Great!<SEP>\"\n",
        "      --> [15496, 50257, 17250, 11, 703, 389, 345, 30, 50257, 13681, 0, 50257]\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenate the utterances as a text\n",
        "    text = \"\"\n",
        "    for utterance in utterances:\n",
        "        text += utterance + \"<SEP>\"\n",
        "\n",
        "    # Tokenize the text and encode the tokens\n",
        "    token_ids = tokenizer.encode(text)\n",
        "\n",
        "    # Truncate the token IDs to the max input length\n",
        "    token_ids = token_ids[:tokenizer.max_len_single_sentence]\n",
        "\n",
        "    return token_ids"
      ],
      "metadata": {
        "id": "u_4O94fwpGJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_data = []\n",
        "dialogs = [json.loads(line) for line in open(\"dailydialog.json\")]\n",
        "for dialog in tqdm(dialogs):\n",
        "    # Encode the utterances\n",
        "    token_ids = encode(dialog[\"utterances\"])\n",
        "\n",
        "    # Store the token IDs\n",
        "    train_data.append(token_ids)\n",
        "\n",
        "print(dialogs[0][\"utterances\"])\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "id": "yUaRcaUaLU28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "import torch\n",
        "import shutil\n",
        "\n",
        "# Load a pretrained GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Load the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Ja142m6hVM_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall torchsummary\n",
        "!pip install torch-summary==1.4.4\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "CMTmPz60VPwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"How should I treat cancer?\"\n",
        "encoded_text=tokenizer(text, max_length=160, padding=True, truncation=True, return_tensors='pt')"
      ],
      "metadata": {
        "id": "gT7hEa11VRVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text['input_ids']"
      ],
      "metadata": {
        "id": "JhHKHBKkWLYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, input_size=(32,1024), dtype=[torch.long])"
      ],
      "metadata": {
        "id": "Rh_Q0RA9YElJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embedding=model.transformer.wte.weight\n",
        "word_embedding.shape"
      ],
      "metadata": {
        "id": "NP2NPnklbuch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "    response = openai.Embedding.create(\n",
        "      input=text,\n",
        "      model=model\n",
        "    )\n",
        "    return np.array(response['data'][0]['embedding'])\n",
        "\n",
        "# Example query and passages\n",
        "query = \"What is dense passage retrieval?\"\n",
        "passages = [\n",
        "    \"Dense Passage Retrieval is a technique that uses dense vector representations for retrieving documents.\",\n",
        "    \"GPT-3 is a large language model trained by OpenAI, known for generating human-like text.\",\n",
        "    \"OpenAI's API provides embeddings that can be used for various NLP tasks.\",\n",
        "    \"Passage retrieval involves finding the most relevant document or text snippet for a given query.\"\n",
        "]\n",
        "\n",
        "# Generate the embedding for the query\n",
        "query_embedding = get_embedding(query)\n",
        "\n",
        "# Generate embeddings for each passage\n",
        "passage_embeddings = [get_embedding(passage) for passage in passages]"
      ],
      "metadata": {
        "id": "MI5bz3-DhMRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hyperparameters (Adjust as you need)\n",
        "max_data_size = 300\n",
        "n_epochs = 2\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    losses = []\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        train_data[:max_data_size],\n",
        "        desc=f\"Epoch {epoch+1}/{n_epochs}\"\n",
        "    )\n",
        "    for instance in progress_bar:\n",
        "        # Prepare input and output tensors\n",
        "        input_tokens = torch.LongTensor(instance).unsqueeze(0).to(device)\n",
        "        output_tokens = torch.LongTensor(instance).unsqueeze(0).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(input_tokens, labels=output_tokens)\n",
        "        loss = output.loss\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Save and display the loss\n",
        "        losses.append(loss.item())\n",
        "        progress_bar.set_postfix(loss=(sum(losses) / len(losses)))\n",
        "\n",
        "    # Save the model checkpoint\n",
        "    torch.save(model.state_dict(), f\"model_checkpoint.pt\")"
      ],
      "metadata": {
        "id": "xfaP4ck6n9Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Information"
      ],
      "metadata": {
        "id": "Zak4daW0cYyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### API and JSON\n",
        "\n",
        "API is a way to give and take data between programms. It is lines of \"code.\"\n",
        "\n",
        "public/private/partner\n",
        "\n",
        "JSON is a specific form to contain complicated data. (e.g. XML, CSV, JSON)\n",
        "\n",
        "-XML < dataname > value < /dataname >\n",
        "\n",
        "-CSV Year, Brand, Model\n",
        "    \n",
        "     1997, Ford, E350\n",
        "\n",
        "-> Good for getting data from database\n",
        "\n",
        "-JSON { \"name\" : \"Tim\", \"age\" : 20 }\n",
        " this is called object in javascript\n",
        "\n",
        " There are many other ways to store data : YAML BSON SMILE etc...\n"
      ],
      "metadata": {
        "id": "-egaAdnjsx6u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lzEYwfVYzyiT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}